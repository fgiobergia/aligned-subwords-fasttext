{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matrix(lang, step=5000):\n",
    "    \n",
    "    print(\"LANG\", lang)\n",
    "    # not aligned\n",
    "    try:\n",
    "        src = load_facebook_vectors(f\"/data1/malto/csavelli/aligned_subwords_fasttext/wiki/wiki.{lang}.bin\") \n",
    "        print(\"Loaded fastText vectors\")\n",
    "    except:\n",
    "        print(\"Going to 'vec'\")\n",
    "        src = KeyedVectors.load_word2vec_format(f\"/data1/malto/csavelli/aligned_subwords_fasttext/wiki/wiki.{lang}.vec\")\n",
    "    dst = KeyedVectors.load_word2vec_format(f\"/data1/malto/csavelli/aligned_subwords_fasttext/aligned/wiki.{lang}.align.vec\") # aligned\n",
    "    \n",
    "    if src.index_to_key != dst.index_to_key:\n",
    "        print(\"src and dst vocabularies differ. \")\n",
    "        print(\"src\", len(src))\n",
    "        print(\"dst\", len(dst))\n",
    "        print(\"in src, not in dst\", set(src.index_to_key) - set(dst.index_to_key))\n",
    "        print(\"in dst, not in src\", set(dst.index_to_key) - set(src.index_to_key))\n",
    "    \n",
    "    vocab = list(set(src.index_to_key) & set(dst.index_to_key))\n",
    "        \n",
    "    Y = dst[vocab]\n",
    "    X = src[vocab]\n",
    "\n",
    "    W_ = np.linalg.pinv(X) @ Y\n",
    "\n",
    "    prod = (X @ W_)\n",
    "    prod = prod / np.linalg.norm(prod, axis=1).reshape(-1,1)\n",
    "\n",
    "    error_couples = []\n",
    "    right_values = []\n",
    "\n",
    "    for i in range(0, len(prod), step):\n",
    "            M = (prod[i:i+step] @ Y.T)\n",
    "            v = M.argmax(axis=1)\n",
    "\n",
    "            # sum of the diagonal\n",
    "            right_values.append(np.diagonal(M[:,v]))\n",
    "            for j in range(len(v)):\n",
    "                if v[j] != i+j: # check that the most vector is the word itself\n",
    "                    print(\"words do not match\", i+j, v[j], M[j,v[j]])\n",
    "                    print(\"instead the right word should be \", M[j,j])\n",
    "                    error_couples.append((i+j, v[j], M[j,v[j]], M[j,j]))\n",
    "\n",
    "                if M[j,v[j]] < .98:\n",
    "                    print(\"small similarity\" , i+j, v[j], M[j,v[j]])\n",
    "    \n",
    "    return src, dst, X, Y, W_, right_values.mean(), right_values.std(), error_couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, dst, X, Y, W_, mean, std, error_couples = find_matrix(\"it\")\n",
    "\n",
    "# save the values \n",
    "import pickle\n",
    "with open(\"it.pkl\", \"wb\") as f:\n",
    "    pickle.dump((src, dst, X, Y, W_, mean, std, error_couples), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, dst, X, Y, W_, mean, std, error_couples = find_matrix(\"it\")\n",
    "\n",
    "with open(\"it.pkl\", \"wb\") as f:\n",
    "    pickle.dump((src, dst, X, Y, W_, mean, std, error_couples), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"res/it.pkl\", \"rb\") as f:\n",
    "    X, Y, W_, mean, std, error_couples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape, W_.shape, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"it\"\n",
    "\n",
    "try:\n",
    "    src = load_facebook_vectors(f\"/data1/malto/csavelli/aligned_subwords_fasttext/wiki/wiki.{lang}.bin\") \n",
    "    print(\"Loaded fastText vectors\")\n",
    "except:\n",
    "    print(\"Going to 'vec'\")\n",
    "    src = KeyedVectors.load_word2vec_format(f\"/data1/malto/csavelli/aligned_subwords_fasttext/wiki/wiki.{lang}.vec\")\n",
    "dst = KeyedVectors.load_word2vec_format(f\"/data1/malto/csavelli/aligned_subwords_fasttext/aligned/wiki.{lang}.align.vec\") # aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src.vectors_ngrams)\n",
    "\n",
    "src.vectors.shape, src.vectors_ngrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.index_to_key[1000], src.buckets_word[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [] \n",
    "for i, n in enumerate(src.buckets_word): \n",
    "    # if in the list there is at least one index in src.buckets_word[1000], take the index \n",
    "    if len(set(n) & set(src.buckets_word[1000])) == 0 and i > 1000:\n",
    "        list.append(i)\n",
    "    if len(list) > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list:\n",
    "    print(src.index_to_key[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "test = deepcopy(src.vectors_ngrams)\n",
    "print(test.shape)\n",
    "\n",
    "# change the vectors of the ngrams\n",
    "for i in range(10):\n",
    "    src.vectors_ngrams[i] = np.random.rand(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0] == src.vectors_ngrams[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 19/01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANG it\n",
      "Loaded fastText vectors\n"
     ]
    }
   ],
   "source": [
    "lang = \"it\"\n",
    "\n",
    "print(\"LANG\", lang)\n",
    "# not aligned\n",
    "try:\n",
    "    src1 = load_facebook_vectors(f\"/data1/malto/csavelli/aligned_subwords_fasttext/wiki/wiki.{lang}.bin\") \n",
    "    print(\"Loaded fastText vectors\")\n",
    "except:\n",
    "    print(\"Going to 'vec'\")\n",
    "    src1 = KeyedVectors.load_word2vec_format(f\"/data1/malto/csavelli/aligned_subwords_fasttext/wiki/wiki.{lang}.vec\")\n",
    "dst1 = KeyedVectors.load_word2vec_format(f\"/data1/malto/csavelli/aligned_subwords_fasttext/aligned/wiki.{lang}.align.vec\") # aligned\n",
    "\n",
    "if src1.index_to_key != dst1.index_to_key:\n",
    "    print(\"src and dst1 vocabularies differ. \")\n",
    "    print(\"src\", len(src1))\n",
    "    print(\"dst1\", len(dst1))\n",
    "    print(\"in src, not in dst1\", set(src1.index_to_key) - set(dst1.index_to_key))\n",
    "    print(\"in dst1, not in src1\", set(dst1.index_to_key) - set(src1.index_to_key))\n",
    "\n",
    "vocab = list(set(src1.index_to_key) & set(dst1.index_to_key))\n",
    "    \n",
    "Y1 = dst1[vocab]\n",
    "X1 = src1[vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"res/it.pkl\", \"rb\") as f:\n",
    "    src, dst, X, Y, W_, mean, std, error_couples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dst[1] == dst1[1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X == X1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, False, False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = src[vocab]\n",
    "X3 = src1[vocab]\n",
    "(X2 == X1).all(), (X3 == X1).all(), (X2 == X3).all(), (X2 == X).all(), (X3 == X).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1637645, 0.14197187)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][2], X1[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chiedessero', 'chiedessero')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(src1.index_to_key) & set(dst1.index_to_key))\n",
    "vocab1 = list(set(src.index_to_key) & set(dst.index_to_key))\n",
    "\n",
    "vocab[0], vocab1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = (X @ W_)\n",
    "prod_norm = prod / np.linalg.norm(prod, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = []\n",
    "v2 = []\n",
    "for elem in error_couples:\n",
    "    v1.append(elem[0])\n",
    "    v2.append(elem[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39181 2.2302065 0.99999994\n",
      "58454 2.7192235 0.99999994\n",
      "65725 2.8147001 1.0\n",
      "140544 2.7426577 1.0\n",
      "167916 2.6931403 0.99999994\n",
      "199118 1.256816 1.0\n",
      "264206 1.3493338 0.99999994\n",
      "266438 2.4148433 1.0\n",
      "352711 2.2067237 1.0\n",
      "362468 2.8288927 0.99999994\n",
      "435193 1.8902451 1.0\n",
      "447450 2.1797674 1.0\n",
      "460780 2.3000891 0.99999994\n",
      "475714 2.8418713 1.0\n",
      "581460 2.7993863 1.0\n",
      "607079 1.9603934 0.99999994\n",
      "627924 2.4457936 1.0\n",
      "703267 2.3468404 1.0\n",
      "716248 2.2692192 1.0\n",
      "750082 2.0722136 0.99999994\n",
      "755119 2.8751884 0.9999999\n",
      "757323 2.7821946 1.0\n",
      "764427 1.3468803 0.99999994\n",
      "778784 1.2880855 1.0\n",
      "790721 2.0233724 1.0\n",
      "814669 2.3137376 1.0\n",
      "837977 1.3491669 0.99999994\n",
      "851996 1.3649734 0.99999994\n",
      "866277 2.1138105 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in v1: \n",
    "    print(i, np.linalg.norm(prod[i]), np.linalg.norm(prod_norm[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703267 2.3468404 1.0\n",
      "412293 2.7636247 0.99999994\n",
      "475714 2.8418713 1.0\n",
      "412293 2.7636247 0.99999994\n",
      "140544 2.7426577 1.0\n",
      "528129 1.2711023 1.0\n",
      "272297 1.3820925 0.99999994\n",
      "736928 2.4621701 1.0\n",
      "703267 2.3468404 1.0\n",
      "475714 2.8418713 1.0\n",
      "607079 1.9603934 0.99999994\n",
      "460780 2.3000891 0.99999994\n",
      "627924 2.4457936 1.0\n",
      "514556 2.8651068 1.0\n",
      "475714 2.8418713 1.0\n",
      "790721 2.0233724 1.0\n",
      "736928 2.4621701 1.0\n",
      "736928 2.4621701 1.0\n",
      "627924 2.4457936 1.0\n",
      "447450 2.1797674 1.0\n",
      "502644 2.9568188 1.0\n",
      "65725 2.8147001 1.0\n",
      "475953 1.3878704 0.99999994\n",
      "602250 1.3073055 0.99999994\n",
      "866277 2.1138105 1.0\n",
      "736928 2.4621701 1.0\n",
      "446447 1.3667009 0.99999994\n",
      "412384 1.4147501 1.0\n",
      "352711 2.2067237 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in v2: \n",
    "    print(i, np.linalg.norm(prod[i]), np.linalg.norm(prod_norm[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energica polylithionite\n",
      "imax glystra\n",
      "producing brinkhues\n",
      "cassettonato glystra\n",
      "spaghetto cassettonato\n",
      "nabla himeshima\n",
      "eucaliptus haldir\n",
      "lecourt manocchi\n",
      "chrebet polylithionite\n",
      "dribblatore brinkhues\n",
      "benedikte curatuli\n",
      "zayton carimi\n",
      "carimi σαν\n",
      "brinkhues einn\n",
      "thickets brinkhues\n",
      "curatuli notoryctidae\n",
      "σαν manocchi\n",
      "polylithionite manocchi\n",
      "└──>roberto σαν\n",
      "ghdimes zayton\n",
      "dissberger lashkargah\n",
      "nfor producing\n",
      "benici chūshirō\n",
      "sučkoŭ tolina\n",
      "notoryctidae qodirov\n",
      "mentenere manocchi\n",
      "nanoindentatore transmutations\n",
      "piarista e^\\,dx\n",
      "qodirov chrebet\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(v1, v2): \n",
    "    print(dst.index_to_key[i], dst.index_to_key[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
